{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from scipy.stats import beta\n",
    "from scipy import special\n",
    "from scipy import optimize\n",
    "import pymc3 as pm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first plot the data to see what everything looks like. From the plot below, we see that var2 and var3 are obviously linearly related. There is maybe a slight linear relation among Y and var1, but not obviously for the other variables. Y is obviously not a logit function, nor does it look to have a gaussian dependence on the different variables. Based on the data, we will assume that we are fitting a linear model $Y = \\alpha_0 + \\alpha_1\\,var1 + \\alpha_2\\,var2 + \\alpha_3\\,var3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ProjectData.csv', delimiter = '\\t')\n",
    "pd.plotting.scatter_matrix(df)\n",
    "plt.savefig('ScatterPlot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = pd.DataFrame(columns = ['std_y', 'std_x1', 'std_x2', 'std_x3'])\n",
    "def standardize(series):\n",
    "    return (series - series.mean())/(series.std())\n",
    "for std_col, col in zip(df_std.columns, df.columns):\n",
    "    df_std[std_col] = standardize(df[col])\n",
    "df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_std['std_x1']\n",
    "X2 = df_std['std_x2']\n",
    "X3 = df_std['std_x3']\n",
    "Y = df_std['std_y']\n",
    "pd.plotting.scatter_matrix(df_std, diagonal = 'kde')\n",
    "#plt.savefig('ScatterPlot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "pearsonr(X2, X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple model: a single gaussian fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# think about if Bayesian stats suppose the \\alpha and \\betas have distributions, or if the x's do. That is, because\n",
    "# either x's or the variables follow normal distributions, that implies so will y.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a simple linear regression for the data here, with the model $Y = \\alpha + \\beta1*x1 + \\beta2*var2 + \\beta3*var3$. The frequentist approach would just fit the data to extract $\\alpha$ and the $\\beta_i$s. The Bayesian way is to give each variable ($\\alpha, \\beta_i$) a distribution, and via Monte Carlo simulations we should be able to model the spread in the data as well as the mean. If we assume these distributions follow $\\alpha = N(\\bar{\\alpha}, \\sigma_1)$, $\\beta_i = N(\\bar{\\beta_i}, \\sigma_i)$, then for this simple model, $Y \\sim N(\\mu, \\sigma^2)$. Here, $\\mu(\\vec{x}) = \\bar{\\alpha} + \\bar{\\beta1}*var1 + \\bar{\\beta2}*var2 + \\bar{\\beta3}*var3$, and $\\sigma = $ (see Vanja's notes first few classes). We weakly inform our priors based on the scatter plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal\n",
    "\n",
    "with pm.Model() as basic_model:\n",
    "    # Priors\n",
    "    alpha = Normal('alpha', mu=0, sd=1)\n",
    "    beta1 = Normal('beta1', mu=0, sd=1)\n",
    "    beta2 = Normal('beta2', mu=0, sd=1)\n",
    "    beta3 = Normal('beta3', mu=0, sd=1)\n",
    "    sigma = HalfNormal('sigma', sd=1)\n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta1*X1 + beta2*X2 + beta3*X3\n",
    "    y_obs = Normal('y_obs', mu=mu, sd=sigma, observed=Y)\n",
    "    \n",
    "    trace = pm.sample(10000, chains=2)\n",
    "    \n",
    "    # MAP estimate of parameters:\n",
    "#    mean = pm.find_MAP(model=basic_model)\n",
    "#    hess = pm.find_hessian(mean, vars = [alpha, beta1, beta2, beta3])\n",
    "#    cov = np.linalg.inv(hess)\n",
    "#    print(mean)\n",
    "#    print(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize = (10,10))\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(trace['alpha'], ax = ax[0,0], use_vlines = True, lags = 50)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[0,0].set_ylabel('AC')\n",
    "ax[0,0].set_xlabel('Lag')\n",
    "ax[0,0].set_title('alpha')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(trace['beta1'], ax = ax[0,1], use_vlines = True, lags = 50)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[0,1].set_ylabel('AC')\n",
    "ax[0,1].set_xlabel('Lag')\n",
    "ax[0,1].set_title('beta1')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(trace['beta3'], ax = ax[1,0], use_vlines = True, lags = 50)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[1,0].set_ylabel('AC')\n",
    "ax[1,0].set_xlabel('Lag')\n",
    "ax[1,0].set_title('beta2')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(trace['beta3'], ax = ax[1,1], use_vlines = True, lags = 50)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[1,1].set_ylabel('AC')\n",
    "ax[1,1].set_xlabel('Lag')\n",
    "ax[1,1].set_title('beta3')\n",
    "\n",
    "plt.savefig('Simplemodel_ACF.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace[::10]);\n",
    "plt.savefig('SimpleModel_ParamResults.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace[::10]);\n",
    "plt.savefig('SimpleModel_ParamResultsb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at how well our simulation models the observed data, including the mean and the tails. To do this, we simulate the predictive posterior for 5000 different combinations of alpha, beta_i (each of length ~100) and look at the histogram. We can also look at how the mean of the predictive posterior (for different alpha, beta_i) compares to the mean of our data. From these, we can see that the single gaussian model correctly determines the mean but the tails are not well modeled. Because of this, we instead move on to a mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with basic_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace[::10], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y, hist=False, kde=True, \n",
    "             bins=40, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Data'})\n",
    "\n",
    "sns.distplot(np.transpose(ppc['y_obs']).flatten(), hist=False, kde=True, \n",
    "             bins=40, color = 'orange', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Predictive Posterior'}, axlabel = None)\n",
    "\n",
    "plt.savefig('SimpleModel_Yhist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can judge the results of our fit by a few methods. One is to look at how well our simulated results predict the mean of y. To do this, we calculate the predictive posterior. This code picks parameters of alpha and beta given our trace and generates ~100 samples given those values of alpha and beta. We do this for 500 different parameter values. We then look at the mean of each of those chains and compare it to the mean of the data. We see below that they are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist([n.mean() for n in ppc['y_obs']], bins=19, alpha=0.5)\n",
    "ax.axvline(Y.mean(), color = 'blue')\n",
    "#ax.hist(df['Y'], bins = 19, alpha = 0.5)\n",
    "ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');\n",
    "plt.savefig('SimpleModel_Meancomparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverting back to original scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".36*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Y'].std(), df['var1'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stats.stackexchange.com/questions/74622/converting-standardized-betas-back-to-original-variables\n",
    "def UnStandardize(intercept, slopes, sigma, or_Y, or_xs):\n",
    "    '''\n",
    "    takes the trace results for the intercept and slopes of an MCMC algorithm using standardized data, as well\n",
    "    as the original unstandardized data (output or_Y, inputs or_xs), and returns the scaled results upon which\n",
    "    means, standard deviations, and HPDs can be calculated.\n",
    "    Assumes regression is Y ~ N(mu, sigma^2)\n",
    "    '''\n",
    "    suma = [slopes[i]*or_Y.std()/or_xs[i].std()*or_xs[i].mean() for i in range(len(or_xs))]\n",
    "    intercept_orig = intercept*or_Y.std() + or_Y.mean() - sum(suma, axis=0)\n",
    "    slopes_orig = [slopes[i]*or_Y.std()/or_xs[i].std() for i in range(len(or_xs))]\n",
    "    sigma_orig = sigma*or_Y.std()\n",
    "    \n",
    "    return np.array(sigma_orig), np.array(intercept_orig), np.array(slopes_orig)\n",
    "\n",
    "intercept_simple = trace[::10]['alpha']\n",
    "slope_var1_simple = trace[::10]['beta1']\n",
    "slope_var2_simple = trace[::10]['beta2']\n",
    "slope_var3_simple = trace[::10]['beta3']\n",
    "sigma_simple = trace[::10]['sigma']\n",
    "sigma_orig, int_orig, slopes_orig = UnStandardize(intercept_simple, np.array([slope_var1_simple, slope_var2_simple, slope_var3_simple]), sigma_simple, df['Y'], np.array([df['var1'],df['var2'], df['var3']]))\n",
    "#int_mean, slopes_mean = UnStandardize(intercept_simple.mean(), np.array([slope_var1_simple.mean(), slope_var2_simple.mean(), slope_var3_simple.mean()]), df['Y'], np.array([df['var1'],df['var2'], df['var3']]))\n",
    "#print(int_mean, slopes_mean)\n",
    "means = [int_orig.mean(), slopes_orig[0].mean(),slopes_orig[1].mean(),slopes_orig[2].mean(), sigma_orig.mean()]\n",
    "stds = [int_orig.std(), slopes_orig[0].std(),slopes_orig[1].std(),slopes_orig[2].std(), sigma_orig.std()]\n",
    "hpds_l = [pm.stats.hpd(int_orig)[0],pm.stats.hpd(slopes_orig[0])[0],pm.stats.hpd(slopes_orig[1])[0],pm.stats.hpd(slopes_orig[2])[0],pm.stats.hpd(sigma_orig)[0]]\n",
    "hpds_h = [pm.stats.hpd(int_orig)[1],pm.stats.hpd(slopes_orig[0])[1],pm.stats.hpd(slopes_orig[1])[1],pm.stats.hpd(slopes_orig[2])[1],pm.stats.hpd(sigma_orig)[1]]\n",
    "\n",
    "data = {'mean': means, 'std': stds, 'hpd_2.5': hpds_l, 'hpd_97.5': hpds_h}\n",
    "trace_unst = pd.DataFrame(data=data, columns = ['mean', 'std', 'hpd_2.5', 'hpd_97.5'], index = ['alpha', 'beta1', 'beta2', 'beta3', 'sigma'])\n",
    "trace_unst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with non-standardized trace results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal\n",
    "\n",
    "with pm.Model() as basic_model_unstd:\n",
    "    # Priors\n",
    "    alpha = Normal('alpha', mu=0, sd=10)\n",
    "    beta1 = Normal('beta1', mu=0, sd=10)\n",
    "    beta2 = Normal('beta2', mu=0, sd=10)\n",
    "    beta3 = Normal('beta3', mu=0, sd=10)\n",
    "    sigma = HalfNormal('sigma', sd=10)\n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta1*df['var1'] + beta2*df['var2'] + beta3*df['var3']\n",
    "    y_obs = Normal('y_obs', mu=mu, sd=sigma, observed=df['Y'])\n",
    "    \n",
    "    trace_unstd = pm.sample(10000, chains=2)\n",
    "    \n",
    "    # MAP estimate of parameters:\n",
    "#    mean = pm.find_MAP(model=basic_model)\n",
    "#    hess = pm.find_hessian(mean, vars = [alpha, beta1, beta2, beta3])\n",
    "#    cov = np.linalg.inv(hess)\n",
    "#    print(mean)\n",
    "#    print(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with basic_model_unstd:\n",
    "    ppc_unstd = pm.sample_posterior_predictive(trace_unstd[::10], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Y'], hist=False, kde=True, \n",
    "             bins=40, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Data'})\n",
    "\n",
    "sns.distplot(np.transpose(ppc_unstd['y_obs']).flatten(), hist=False, kde=True, \n",
    "             bins=40, color = 'orange', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Predictive Posterior'}, axlabel = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_unstd[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "xs = np.linspace(-3,4,100)\n",
    "ys = .25*scipy.stats.norm.pdf(xs, 1.05, .5) + .65*scipy.stats.norm.pdf(xs, -.45, .8)\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.distplot(Y, hist=False, kde=True, \n",
    "             bins=40, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Data'},ax=ax)\n",
    "ax.plot(xs,ys, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first try 2 mixture components to test out the mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = pd.DataFrame(columns = ['std_y', 'std_x1', 'std_x2', 'std_x3'])\n",
    "def standardize(series):\n",
    "    return (series - series.mean())/(series.std())\n",
    "for std_col, col in zip(df_std.columns, df.columns):\n",
    "    df_std[std_col] = standardize(df[col])\n",
    "df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_std)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = df_std['std_x1']\n",
    "X2 = df_std['std_x2']\n",
    "X3 = df_std['std_x3']\n",
    "Y = df_std['std_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tighter restrictions on priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal, Beta\n",
    "from theano import tensor as tt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pm.Model() as mixture_model_b:\n",
    "    # Priors\n",
    "    \n",
    " #   BoundedPrior = pm.Bound(pm.Cauchy, lower = -5, upper = 5)\n",
    " #   alpha_a = BoundedPrior('alpha_a', alpha=1.15, beta=2.5, shape = 2)\n",
    " #   alpha_b = BoundedPrior('alpha_b', alpha=-.5, beta=2.5, shape = 2)\n",
    " #   beta1 = BoundedPrior('beta1', alpha=0, beta=0.75, shape = 2)\n",
    " #   beta2 = BoundedPrior('beta2', alpha=0, beta=0.75, shape = 2)\n",
    "#  beta3 = pm.Cauchy('beta3', alpha=0, beta=0.75, shape = 2)\n",
    "#    BoundedNoise = pm.Bound(pm.HalfCauchy, lower = 0, upper = 4)\n",
    "#    sigma = BoundedNoise('sigma', beta=0.75, shape = 2)\n",
    "    \n",
    " #   alpha_a = pm.Cauchy('alpha_a', alpha=0, beta=2.5, shape = 1)\n",
    " #   alpha_b = pm.Cauchy('alpha_b', alpha=0, beta=2.5, shape = 1)\n",
    "    alpha = pm.Cauchy('alpha', alpha=0, beta=1.2, shape=2)\n",
    "    beta1 = pm.Cauchy('beta1', alpha=0, beta=1.2, shape = 2)\n",
    "    beta2 = pm.Cauchy('beta2', alpha=0, beta=1.2, shape = 2)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1.0, shape = 2)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = tt.stack([alpha[0] + beta1[0]*X1 + beta2[0]*X2, \n",
    "                   alpha[1] + beta1[1]*X1 + beta2[1]*X2], axis = 1)\n",
    "    \n",
    "    order_means_potential = pm.Potential('order_means_potential', tt.switch(mu[0] - mu[1] < 0, -np.inf, 0))\n",
    "    \n",
    "    # weights\n",
    "#    BoundedWeights = pm.Bound(pm.Dirichlet, lower=0.1, upper = 0.9)\n",
    "   # BoundedWeights = pm.Bound(pm.Beta, lower=0.1, upper = 0.9)\n",
    "   # beta = BoundedWeights('beta', alpha = 7., beta = 3, shape = 2)\n",
    " #   w = np.array([beta, 1-beta])\n",
    "    w = pm.Dirichlet('w', np.array([1, 1]))\n",
    "#    w = BoundedWeights('w', a = np.array([1,1]))\n",
    "\n",
    "    y_obs = pm.NormalMixture('y_obs', w, mu, sd=sigma, comp_shape = (2,), observed=Y)\n",
    "    \n",
    "#    step = pm.Metropolis()\n",
    "    trace2b = pm.sample(10000, init = 'advi_map', chains=1, random_seed = 2, nuts_kwargs=dict(target_accept=.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,3))\n",
    "ax = pm.traceplot(trace2b[:9500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying 2 long chains..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal, Beta\n",
    "from theano import tensor as tt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pm.Model() as mixture_model_c:\n",
    "    # Priors\n",
    "    alpha = pm.Cauchy('alpha', alpha=0, beta=1.2, shape=2)\n",
    "    beta1 = pm.Cauchy('beta1', alpha=0, beta=1.2, shape = 2)\n",
    "    beta2 = pm.Cauchy('beta2', alpha=0, beta=1.2, shape = 2)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1.0, shape = 2)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = tt.stack([alpha[0] + beta1[0]*X1 + beta2[0]*X2, \n",
    "                   alpha[1] + beta1[1]*X1 + beta2[1]*X2], axis = 1)\n",
    "    \n",
    "    order_means_potential = pm.Potential('order_means_potential', tt.switch(mu[0] - mu[1] < 0, -np.inf, 0))\n",
    "    \n",
    "    # weights\n",
    "    w = pm.Dirichlet('w', np.array([1, 1]))\n",
    "\n",
    "    y_obs = pm.NormalMixture('y_obs', w, mu, sd=sigma, comp_shape = (2,), observed=Y)\n",
    "    \n",
    "    trace2c = pm.sample(10000, init = 'advi_map', chains=2, nuts_kwargs=dict(target_accept=.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(trace2c[1500:8000])\n",
    "ax[0,0].set_xlim(-2,2)\n",
    "ax[0,1].set_ylim(-3,3)\n",
    "ax[1,0].set_xlim(-1,1)\n",
    "ax[1,1].set_ylim(-0,1)\n",
    "ax[2,0].set_xlim(-1,1)\n",
    "ax[2,1].set_ylim(-.5,.5)\n",
    "ax[3,0].set_xlim(0,2)\n",
    "ax[3,1].set_ylim(-.5,1.5)\n",
    "plt.savefig('Mixture_Component_Results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in from 0 -> 4500, things look like they're in 1 mode. I'll use this section for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(trace2c[1500:4500])\n",
    "ax[0,0].set_xlim(-2,2)\n",
    "ax[0,1].set_ylim(-3,3)\n",
    "ax[1,0].set_xlim(-1,1)\n",
    "ax[1,1].set_ylim(-0,1)\n",
    "ax[2,0].set_xlim(-1,1)\n",
    "ax[2,1].set_ylim(-.5,.5)\n",
    "ax[3,0].set_xlim(0,2)\n",
    "ax[3,1].set_ylim(-.5,1.5)\n",
    "plt.savefig('Mixture_Component_Results_zoom.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACF/Thinning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to only look at 1 mode, or else you'll have huge ACFs!\n",
    "fig, ax = plt.subplots(2,2, figsize = (10,10))\n",
    "\n",
    "max_lag=300\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2c['alpha'])[0][2000:4500], ax = ax[0,0], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[0,0].set_ylabel('AC')\n",
    "ax[0,0].set_xlabel('Lag')\n",
    "ax[0,0].set_title('alpha')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2c['beta1'])[0][2000:4500], ax = ax[0,1], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[0,1].set_ylabel('AC')\n",
    "ax[0,1].set_xlabel('Lag')\n",
    "ax[0,1].set_title('beta1')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2c['beta2'])[1][2000:4500], ax = ax[1,0], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[1,0].set_ylabel('AC')\n",
    "ax[1,0].set_xlabel('Lag')\n",
    "ax[1,0].set_title('beta2')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2c['alpha'])[1][2000:4500], ax = ax[1,1], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[1,1].set_ylabel('AC')\n",
    "ax[1,1].set_xlabel('Lag')\n",
    "ax[1,1].set_title('alpha')\n",
    "\n",
    "#plt.savefig('Mixture_ACF.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.diagnostics.gelman_rubin(trace2c[2000:4500][::50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace2c[2000:4500][::50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace2c[1500:4500][::50]);\n",
    "plt.savefig('Mixture_ParamResultsb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of fit: simulating the posterior predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_approx = pm.Empirical(trace2c[1500:4500][::50], model=mixture_model_c)\n",
    "pm.plot_posterior(trace_approx.sample(5000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mixture_model_c:\n",
    "    ppc_2chain_b = pm.sample_posterior_predictive(trace2c[2000:4500][::50], 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mixture_model_c:\n",
    "    ppc_2chain_b = pm.sample_posterior_predictive(trace2c[2000:4500][::50], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mixture_model_c:\n",
    "    ppc_2chain = pm.sample_posterior_predictive(trace2c[1500:4500][::50], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y, hist=False, kde=True, \n",
    "             bins=40, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Data'})\n",
    "\n",
    "sns.distplot(np.transpose(ppc_2chain_b['y_obs']).flatten(), hist=False, kde=True, \n",
    "             bins=40, color = 'orange', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4, 'label': 'Predictive Posterior'}, axlabel = None)\n",
    "\n",
    "#plt.savefig('Mixture_histogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(Y, bins=40, normed=True,\n",
    "        histtype='step', lw=4,\n",
    "        label='Observed data');\n",
    "ax.hist(np.transpose(ppc_2chain['y_obs']).flatten(), color='orange', bins=100, normed=True,\n",
    "        histtype='step', lw=4,\n",
    "        label='Posterior predictive distribution');\n",
    "#ax.hist(ppc['y_obs'].mean(axis=0), color = 'black', bins=30, normed=True,\n",
    "#        histtype='step', lw=2, alpha = 1.0,\n",
    "#        label='Posterior predictive distribution');\n",
    "\n",
    "ax.legend(loc=1);\n",
    "plt.savefig('Mixture_histogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with mixture_model:\n",
    "    \n",
    "    ppc_2chain_b = pm.sample_posterior_predictive(trace2c[2000:4500][::50], 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp=pm.compare({basic_model: trace[::10], mixture_model_c: trace2c[1500:4500][::50]})\n",
    "df_comp\n",
    "#indicates mixture is a better fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picked priors $\\alpha, \\beta1, \\beta2 \\sim Cauchy(0, 1.25)$, $\\sigma \\sim HalfCauchy(1)$. We will try to change the mean and variance of these priors and via importance sampling back out how sensitive our results are to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First change $\\alpha[0] \\sim Cauchy(1, 2.5) , \\alpha[1] \\sim Cauchy(-.5, 2.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transposetrace2c['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(alpha0, alpha1):\n",
    "    prior1 = stats.cauchy.pdf(alpha0,0,1.25)*stats.cauchy.pdf(alpha1, 0, 1.25)\n",
    "    priornew = stats.cauchy.pdf(alpha0, 1, 10)*stats.cauchy.pdf(alpha1,-0.5,10)\n",
    "    w = priornew/prior1\n",
    "    return w\n",
    "\n",
    "w = weights(np.transpose(trace2c['alpha'])[0][1500:4500][::50], np.transpose(trace2c['alpha'])[1][1500:4500][::50])\n",
    "w = w/sum(w)\n",
    "resample_t = np.random.choice(np.transpose(trace2c['alpha'])[0][1500:4500][::50], size = 2000, replace=True, p=w)\n",
    "resample_t2 = np.random.choice(np.transpose(trace2c['alpha'])[1][1500:4500][::50], size = 2000, replace=True, p=w)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].hist(np.transpose(trace2c['alpha'])[0][1500:4500][::50], lw = 2, histtype = 'step', normed=True, color = 'orange')\n",
    "ax[0].hist(resample_t, lw = 2, histtype = 'step', color = 'blue', normed=True)\n",
    "ax[0].set_title('Alpha_0 under priors Cauchy(1, 10)')\n",
    "\n",
    "ax[1].hist(np.transpose(trace2c['alpha'])[1][1500:4500][::50], lw = 2, histtype = 'step', normed=True, color = 'orange')\n",
    "ax[1].hist(resample_t2, lw = 2, histtype = 'step', color = 'blue', normed=True)\n",
    "ax[1].set_title('Alpha_1 under priors Cauchy(-.5, 10)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ImportanceSampling.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace2c[1500:4500][::50]['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stats.stackexchange.com/questions/74622/converting-standardized-betas-back-to-original-variables\n",
    "def UnStandardize(intercept, slopes, sigma, or_Y, or_xs):\n",
    "    '''\n",
    "    takes the trace results for the intercept and slopes of an MCMC algorithm using standardized data, as well\n",
    "    as the original unstandardized data (output or_Y, inputs or_xs), and returns the scaled results upon which\n",
    "    means, standard deviations, and HPDs can be calculated.\n",
    "    Assumes regression is Y ~ N(mu, sigma^2)\n",
    "    '''\n",
    "    suma = [slopes[i]*or_Y.std()/or_xs[i].std()*or_xs[i].mean() for i in range(len(or_xs))]\n",
    "    intercept_orig = intercept*or_Y.std() + or_Y.mean() - sum(suma, axis=0)\n",
    "    slopes_orig = [slopes[i]*or_Y.std()/or_xs[i].std() for i in range(len(or_xs))]\n",
    "    sigma_orig = sigma*or_Y.std()\n",
    "    \n",
    "    return np.array(sigma_orig), np.array(intercept_orig), np.array(slopes_orig)\n",
    "\n",
    "intercept_mixture_0 = np.transpose(trace2c[1500:4500][::50]['alpha'])[0]\n",
    "intercept_mixture_1 = np.transpose(trace2c[1500:4500][::50]['alpha'])[1]\n",
    "slope_var1_mixture_0 = np.transpose(trace2c[1500:4500][::50]['beta1'])[0]\n",
    "slope_var1_mixture_1 = np.transpose(trace2c[1500:4500][::50]['beta1'])[1]\n",
    "slope_var2_mixture_0 = np.transpose(trace2c[1500:4500][::50]['beta2'])[0]\n",
    "slope_var2_mixture_1 = np.transpose(trace2c[1500:4500][::50]['beta2'])[1]\n",
    "sigma_mixture_0 = np.transpose(trace2c[1500:4500][::50]['sigma'])[0]\n",
    "sigma_mixture_1 = np.transpose(trace2c[1500:4500][::50]['sigma'])[1]\n",
    "w_0 = np.transpose(trace2c[1500:4500][::50]['w'])[0]\n",
    "w_1 = np.transpose(trace2c[1500:4500][::50]['w'])[1]\n",
    "\n",
    "sigma_orig_0, int_orig_0, slopes_orig_0 = UnStandardize(intercept_mixture_0, np.array([slope_var1_mixture_0, slope_var2_mixture_0]), sigma_mixture_0, df['Y'], np.array([df['var1'],df['var2']]))\n",
    "sigma_orig_1, int_orig_1, slopes_orig_1 = UnStandardize(intercept_mixture_1, np.array([slope_var1_mixture_1, slope_var2_mixture_1]), sigma_mixture_1, df['Y'], np.array([df['var1'],df['var2']]))\n",
    "\n",
    "means = [int_orig_0.mean(), int_orig_1.mean(), slopes_orig_0[0].mean(),slopes_orig_1[0].mean(), slopes_orig_0[1].mean(),slopes_orig_1[1].mean(), sigma_orig_0.mean(),sigma_orig_1.mean(), w_0.mean(), w_1.mean()]\n",
    "stds = [int_orig_0.std(), int_orig_1.std(), slopes_orig_0[0].std(),slopes_orig_1[0].std(), slopes_orig_0[1].std(),slopes_orig_1[1].std(), sigma_orig_0.std(),sigma_orig_1.std(), w_0.std(), w_1.std()]\n",
    "hpds_l = [pm.stats.hpd(int_orig_0)[0], pm.stats.hpd(int_orig_1)[0], pm.stats.hpd(slopes_orig_0[0])[0],pm.stats.hpd(slopes_orig_1[0])[0], pm.stats.hpd(slopes_orig_0[1])[0],pm.stats.hpd(slopes_orig_1[1])[0], pm.stats.hpd(sigma_orig_0)[0],pm.stats.hpd(sigma_orig_1)[0], pm.stats.hpd(w_0)[0], pm.stats.hpd(w_1)[0]]\n",
    "hpds_h = [pm.stats.hpd(int_orig_0)[1], pm.stats.hpd(int_orig_1)[1], pm.stats.hpd(slopes_orig_0[0])[1],pm.stats.hpd(slopes_orig_1[0])[1], pm.stats.hpd(slopes_orig_0[1])[1],pm.stats.hpd(slopes_orig_1[1])[1], pm.stats.hpd(sigma_orig_0)[1],pm.stats.hpd(sigma_orig_1)[1], pm.stats.hpd(w_0)[1], pm.stats.hpd(w_1)[1]]\n",
    "\n",
    "data = {'mean': means, 'std': stds, 'hpd_2.5': hpds_l, 'hpd_97.5': hpds_h}\n",
    "trace_unst_mix = pd.DataFrame(data=data, columns = ['mean', 'std', 'hpd_2.5', 'hpd_97.5'], index = ['alpha_0', 'alpha_1', 'beta1_0', 'beta1_1', 'beta2_0', 'beta2_1', 'sigma_0', 'sigma_1', 'w_0', 'w_1'])\n",
    "trace_unst_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace2c[1500:4500][::50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1_dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = trace2c[1500:4500][::50]\n",
    "def sample_posterior_x1_nosigma(x): # returns a random sample from the posterior for y(x1)\n",
    "    alpha0_mean, alpha0_std = np.transpose(tr['alpha'])[0].mean(), np.transpose(tr['alpha'])[0].std()\n",
    "    beta10_mean, beta10_std = np.transpose(tr['beta1'])[0].mean(), np.transpose(tr['beta1'])[0].std()\n",
    "    w0_mean, w0_std = np.transpose(tr['w'])[0].mean(), np.transpose(tr['w'])[0].std()\n",
    "    sigma0_mean, sigma0_std = np.transpose(tr['sigma'])[0].mean(), np.transpose(tr['sigma'])[0].std()\n",
    "    \n",
    "    # sample parameters randomly based on trace distribution\n",
    "    alpha0_sample = np.random.normal(alpha0_mean, alpha0_std)\n",
    "    beta10_sample = np.random.normal(beta10_mean, beta10_std)\n",
    "    w0_sample = np.random.normal(w0_mean, w0_std)\n",
    "    sigma0_sample = np.abs(np.random.normal(sigma0_mean, sigma0_std))\n",
    "    \n",
    "    \n",
    "    # repeat for comp2\n",
    "    alpha1_mean, alpha1_std = np.transpose(tr['alpha'])[1].mean(), np.transpose(tr['alpha'])[1].std()\n",
    "    beta11_mean, beta11_std = np.transpose(tr['beta1'])[1].mean(), np.transpose(tr['beta1'])[1].std()\n",
    "    sigma1_mean, sigma1_std = np.transpose(tr['sigma'])[1].mean(), np.transpose(tr['sigma'])[1].std()\n",
    "    \n",
    "    alpha1_sample = np.random.normal(alpha1_mean, alpha1_std)\n",
    "    beta11_sample = np.random.normal(beta11_mean, beta11_std)\n",
    "    w1_sample = 1- w0_sample\n",
    "    sigma1_sample = np.abs(np.random.normal(sigma1_mean, sigma1_std))\n",
    "\n",
    " #   for i in X1: # for each Xi, randomly choose with weight w if point comes from distribution 1 or 2\n",
    "    rand_no = np.random.rand()\n",
    "    if rand_no <= w0_sample:\n",
    "        out = alpha0_sample + beta10_sample*x\n",
    "    else:\n",
    "        out = alpha1_sample + beta11_sample*x\n",
    "    \n",
    "    return out\n",
    "\n",
    "def sample_posterior_x1(x): # returns a random sample from the posterior for y(x1)\n",
    "    alpha0_mean, alpha0_std = np.transpose(tr['alpha'])[0].mean(), np.transpose(tr['alpha'])[0].std()\n",
    "    beta10_mean, beta10_std = np.transpose(tr['beta1'])[0].mean(), np.transpose(tr['beta1'])[0].std()\n",
    "    w0_mean, w0_std = np.transpose(tr['w'])[0].mean(), np.transpose(tr['w'])[0].std()\n",
    "    sigma0_mean, sigma0_std = np.transpose(tr['sigma'])[0].mean(), np.transpose(tr['sigma'])[0].std()\n",
    "    \n",
    "    # sample parameters randomly based on trace distribution\n",
    "    alpha0_sample = np.random.normal(alpha0_mean, alpha0_std)\n",
    "    beta10_sample = np.random.normal(beta10_mean, beta10_std)\n",
    "    w0_sample = np.random.normal(w0_mean, w0_std)\n",
    "    sigma0_sample = np.abs(np.random.normal(sigma0_mean, sigma0_std))\n",
    "    \n",
    "    \n",
    "    # repeat for comp2\n",
    "    alpha1_mean, alpha1_std = np.transpose(tr['alpha'])[1].mean(), np.transpose(tr['alpha'])[1].std()\n",
    "    beta11_mean, beta11_std = np.transpose(tr['beta1'])[1].mean(), np.transpose(tr['beta1'])[1].std()\n",
    "    sigma1_mean, sigma1_std = np.transpose(tr['sigma'])[1].mean(), np.transpose(tr['sigma'])[1].std()\n",
    "    \n",
    "    alpha1_sample = np.random.normal(alpha1_mean, alpha1_std)\n",
    "    beta11_sample = np.random.normal(beta11_mean, beta11_std)\n",
    "    w1_sample = 1- w0_sample\n",
    "    sigma1_sample = np.abs(np.random.normal(sigma1_mean, sigma1_std))\n",
    "\n",
    " #   for i in X1: # for each Xi, randomly choose with weight w if point comes from distribution 1 or 2\n",
    "    rand_no = np.random.rand()\n",
    "    if rand_no <= w0_sample:\n",
    "        out = np.random.normal(alpha0_sample + beta10_sample*x, sigma0_sample)\n",
    "    else:\n",
    "        out = np.random.normal(alpha1_sample + beta11_sample*x, sigma1_sample)\n",
    "    \n",
    "    return out\n",
    "\n",
    "pred_comp1 = (np.transpose(tr['alpha'])[0].mean() + np.transpose(tr['beta1'])[0].mean()*X1)\n",
    "pred_comp2 = (np.transpose(tr['alpha'])[1].mean() + np.transpose(tr['beta1'])[1].mean()*X1)\n",
    "pred_1 = np.transpose(tr['w'])[0].mean()*pred_comp1 + np.transpose(tr['w'])[1].mean()*pred_comp2\n",
    "pred_1_dist = [ [sample_posterior_x1(i) for i in xs] for j in range(20) ]\n",
    "xs = np.linspace(-2.5,2.5, 50)\n",
    "pred_1_dist_nosigma = [ [sample_posterior_x1_nosigma(i) for i in xs] for j in range(20) ]\n",
    "\n",
    "\n",
    "pred_comp1b = np.transpose(tr['w'])[0].mean()*(np.transpose(tr['alpha'])[0].mean() + np.transpose(tr['beta1'])[0].mean()*X2)\n",
    "pred_comp2b = np.transpose(tr['w'])[1].mean()*(np.transpose(tr['alpha'])[1].mean() + np.transpose(tr['beta1'])[1].mean()*X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "ax[0,0].plot(X1, pred_1, 'g-', label = 'Average regression')\n",
    "ax[1,0].plot(X1, pred_1, 'g-', label = 'Average regression')\n",
    "i = 0\n",
    "print(len(pred_1_dist))\n",
    "for i in range(len(pred_1_dist)):\n",
    "        ax[0,0].plot(xs, pred_1_dist_nosigma[i], 'go', alpha = .3, label = 'Simulation')\n",
    "        i +=1\n",
    "ax[0,0].plot(X1, Y, 'ro', label = 'Data')\n",
    "ax[0,0].set_xlabel('var1')\n",
    "ax[0,0].set_ylabel('Y')\n",
    "\n",
    "ax[0,1].hist(np.array(pred_1_dist_nosigma).flatten(), orientation = 'horizontal', color = 'green', bins = 30, lw = 3, normed = True)\n",
    "\n",
    "for i in range(len(pred_1_dist)):\n",
    "        ax[1,0].plot(xs, pred_1_dist[i], 'go', alpha = .3)\n",
    "        i +=1\n",
    "ax[1,0].plot(X1, Y, 'ro')\n",
    "ax[1,0].set_xlabel('var1')\n",
    "ax[1,0].set_ylabel('Y')\n",
    "\n",
    "ax[1,1].hist(np.array(pred_1_dist).flatten(), orientation = 'horizontal', color = 'green', bins = 30, lw = 3, normed = True)\n",
    "ax[0,0].set_title('Simulation without noise, Y ~ mu')\n",
    "ax[1,0].set_title('Simulation with noise, Y ~ N(mu, sigma^2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Var1_Simulationresults.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = trace2c[1500:4500][::50]\n",
    "def sample_posterior_x2_nosigma(x): # returns a random sample from the posterior for y(x1)\n",
    "    alpha0_mean, alpha0_std = np.transpose(tr['alpha'])[0].mean(), np.transpose(tr['alpha'])[0].std()\n",
    "    beta20_mean, beta20_std = np.transpose(tr['beta2'])[0].mean(), np.transpose(tr['beta2'])[0].std()\n",
    "    w0_mean, w0_std = np.transpose(tr['w'])[0].mean(), np.transpose(tr['w'])[0].std()\n",
    "    sigma0_mean, sigma0_std = np.transpose(tr['sigma'])[0].mean(), np.transpose(tr['sigma'])[0].std()\n",
    "    \n",
    "    # sample parameters randomly based on trace distribution\n",
    "    alpha0_sample = np.random.normal(alpha0_mean, alpha0_std)\n",
    "    beta20_sample = np.random.normal(beta20_mean, beta20_std)\n",
    "    w0_sample = np.random.normal(w0_mean, w0_std)\n",
    "    sigma0_sample = np.abs(np.random.normal(sigma0_mean, sigma0_std))\n",
    "    \n",
    "    \n",
    "    # repeat for comp2\n",
    "    alpha1_mean, alpha1_std = np.transpose(tr['alpha'])[1].mean(), np.transpose(tr['alpha'])[1].std()\n",
    "    beta21_mean, beta21_std = np.transpose(tr['beta2'])[1].mean(), np.transpose(tr['beta2'])[1].std()\n",
    "    sigma1_mean, sigma1_std = np.transpose(tr['sigma'])[1].mean(), np.transpose(tr['sigma'])[1].std()\n",
    "    \n",
    "    alpha1_sample = np.random.normal(alpha1_mean, alpha1_std)\n",
    "    beta21_sample = np.random.normal(beta21_mean, beta21_std)\n",
    "    w1_sample = 1- w0_sample\n",
    "    sigma1_sample = np.abs(np.random.normal(sigma1_mean, sigma1_std))\n",
    "\n",
    " #   for i in X1: # for each Xi, randomly choose with weight w if point comes from distribution 1 or 2\n",
    "    rand_no = np.random.rand()\n",
    "    if rand_no <= w0_sample:\n",
    "        out = alpha0_sample + beta20_sample*x\n",
    "    else:\n",
    "        out = alpha1_sample + beta21_sample*x\n",
    "    \n",
    "    return out\n",
    "\n",
    "def sample_posterior_x2(x): # returns a random sample from the posterior for y(x1)\n",
    "    alpha0_mean, alpha0_std = np.transpose(tr['alpha'])[0].mean(), np.transpose(tr['alpha'])[0].std()\n",
    "    beta20_mean, beta20_std = np.transpose(tr['beta2'])[0].mean(), np.transpose(tr['beta2'])[0].std()\n",
    "    w0_mean, w0_std = np.transpose(tr['w'])[0].mean(), np.transpose(tr['w'])[0].std()\n",
    "    sigma0_mean, sigma0_std = np.transpose(tr['sigma'])[0].mean(), np.transpose(tr['sigma'])[0].std()\n",
    "    \n",
    "    # sample parameters randomly based on trace distribution\n",
    "    alpha0_sample = np.random.normal(alpha0_mean, alpha0_std)\n",
    "    beta20_sample = np.random.normal(beta20_mean, beta20_std)\n",
    "    w0_sample = np.random.normal(w0_mean, w0_std)\n",
    "    sigma0_sample = np.abs(np.random.normal(sigma0_mean, sigma0_std))\n",
    "    \n",
    "    \n",
    "    # repeat for comp2\n",
    "    alpha1_mean, alpha1_std = np.transpose(tr['alpha'])[1].mean(), np.transpose(tr['alpha'])[1].std()\n",
    "    beta21_mean, beta21_std = np.transpose(tr['beta2'])[1].mean(), np.transpose(tr['beta2'])[1].std()\n",
    "    sigma1_mean, sigma1_std = np.transpose(tr['sigma'])[1].mean(), np.transpose(tr['sigma'])[1].std()\n",
    "    \n",
    "    alpha1_sample = np.random.normal(alpha1_mean, alpha1_std)\n",
    "    beta21_sample = np.random.normal(beta21_mean, beta21_std)\n",
    "    w1_sample = 1- w0_sample\n",
    "    sigma1_sample = np.abs(np.random.normal(sigma1_mean, sigma1_std))\n",
    "\n",
    " #   for i in X1: # for each Xi, randomly choose with weight w if point comes from distribution 1 or 2\n",
    "    rand_no = np.random.rand()\n",
    "    if rand_no <= w0_sample:\n",
    "        out = np.random.normal(alpha0_sample + beta20_sample*x, sigma0_sample)\n",
    "    else:\n",
    "        out = np.random.normal(alpha1_sample + beta21_sample*x, sigma1_sample)\n",
    "    \n",
    "    return out\n",
    "\n",
    "pred_comp1 = (np.transpose(tr['alpha'])[0].mean() + np.transpose(tr['beta1'])[0].mean()*X1)\n",
    "pred_comp2 = (np.transpose(tr['alpha'])[1].mean() + np.transpose(tr['beta1'])[1].mean()*X1)\n",
    "pred_1 = np.transpose(tr['w'])[0].mean()*pred_comp1 + np.transpose(tr['w'])[1].mean()*pred_comp2\n",
    "pred_1_dist = [ [sample_posterior_x1(i) for i in xs] for j in range(20) ]\n",
    "xs = np.linspace(-2.5,2.5, 50)\n",
    "pred_1_dist_nosigma = [ [sample_posterior_x1_nosigma(i) for i in xs] for j in range(20) ]\n",
    "\n",
    "\n",
    "pred_comp1b = np.transpose(tr['w'])[0].mean()*(np.transpose(tr['alpha'])[0].mean() + np.transpose(tr['beta2'])[0].mean()*X2)\n",
    "pred_comp2b = np.transpose(tr['w'])[1].mean()*(np.transpose(tr['alpha'])[1].mean() + np.transpose(tr['beta2'])[1].mean()*X2)\n",
    "pred_2 = np.transpose(tr['w'])[0].mean()*pred_comp1b + np.transpose(tr['w'])[1].mean()*pred_comp2b\n",
    "xs = np.linspace(-2.5,2.5, 50)\n",
    "pred_2_dist = [ [sample_posterior_x2(i) for i in xs] for j in range(20) ]\n",
    "pred_2_dist_nosigma = [ [sample_posterior_x2_nosigma(i) for i in xs] for j in range(20) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "ax[0,0].plot(X2, pred_2, 'g-', label = 'Average regression')\n",
    "ax[1,0].plot(X2, pred_2, 'g-', label = 'Average regression')\n",
    "i = 0\n",
    "print(len(pred_2_dist))\n",
    "for i in range(len(pred_2_dist)):\n",
    "        ax[0,0].plot(xs, pred_2_dist_nosigma[i], 'go', alpha = .3, label = 'Simulation')\n",
    "        i +=1\n",
    "ax[0,0].plot(X2, Y, 'ro', label = 'Data')\n",
    "ax[0,0].set_xlabel('var2')\n",
    "ax[0,0].set_ylabel('Y')\n",
    "\n",
    "ax[0,1].hist(np.array(pred_2_dist_nosigma).flatten(), orientation = 'horizontal', color = 'green', bins = 30, lw = 3, normed=True)\n",
    "\n",
    "for i in range(len(pred_2_dist)):\n",
    "        ax[1,0].plot(xs, pred_2_dist[i], 'go', alpha = .3)\n",
    "        i +=1\n",
    "ax[1,0].plot(X2, Y, 'ro')\n",
    "ax[1,0].set_xlabel('var2')\n",
    "ax[1,0].set_ylabel('Y')\n",
    "\n",
    "ax[1,1].hist(np.array(pred_2_dist).flatten(), orientation = 'horizontal', color = 'green', bins = 30, lw = 3, normed=True)\n",
    "ax[0,0].set_title('Simulation without noise, Y ~ mu')\n",
    "ax[1,0].set_title('Simulation with noise, Y ~ N(mu, sigma^2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Var2_Simulationresults.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressions = ppc_2chain['alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just intercepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal, Beta\n",
    "from theano import tensor as tt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pm.Model() as intercept_model:\n",
    "    # Priors\n",
    "    alpha = pm.Cauchy('alpha', alpha=0, beta=1.2, shape=2)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1.0, shape = 2)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = tt.stack([alpha[0], \n",
    "                   alpha[1]], axis = 1)\n",
    "    \n",
    "    order_means_potential = pm.Potential('order_means_potential', tt.switch(mu[0] - mu[1] < 0, -np.inf, 0))\n",
    "    \n",
    "    # weights\n",
    "    w = pm.Dirichlet('w', np.array([1, 1]))\n",
    "\n",
    "    y_obs = pm.NormalMixture('y_obs', w, mu, sd=sigma, comp_shape = (2,), observed=Y)\n",
    "    \n",
    "#    step = pm.Metropolis()\n",
    "    trace_int = pm.sample(5000, init = 'advi_map', chains=2, nuts_kwargs=dict(target_accept=.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(trace_int[:2000])\n",
    "ax[0,0].set_xlim(-3,3)\n",
    "ax[0,1].set_ylim(-3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace_int[:2000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a longer sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal, Beta\n",
    "from theano import tensor as tt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pm.Model() as mixture_model_long:\n",
    "    # Priors\n",
    "    \n",
    " #   BoundedPrior = pm.Bound(pm.Cauchy, lower = -5, upper = 5)\n",
    " #   alpha_a = BoundedPrior('alpha_a', alpha=1.15, beta=2.5, shape = 2)\n",
    " #   alpha_b = BoundedPrior('alpha_b', alpha=-.5, beta=2.5, shape = 2)\n",
    " #   beta1 = BoundedPrior('beta1', alpha=0, beta=0.75, shape = 2)\n",
    " #   beta2 = BoundedPrior('beta2', alpha=0, beta=0.75, shape = 2)\n",
    "#  beta3 = pm.Cauchy('beta3', alpha=0, beta=0.75, shape = 2)\n",
    "#    BoundedNoise = pm.Bound(pm.HalfCauchy, lower = 0, upper = 4)\n",
    "#    sigma = BoundedNoise('sigma', beta=0.75, shape = 2)\n",
    "    \n",
    "  #  alpha_a = pm.Cauchy('alpha_a', alpha=-0.5, beta=2.5, shape = 1)\n",
    "  #  alpha_b = pm.Cauchy('alpha_b', alpha=1.15, beta=2.5, shape = 1)\n",
    "    alpha = pm.Cauchy('alpha', alpha=0, beta=2.5, shape=2)\n",
    "    beta1 = pm.Cauchy('beta1', alpha=0, beta=2.5, shape = 2)\n",
    "    beta2 = pm.Cauchy('beta2', alpha=0, beta=2.5, shape = 2)\n",
    " #   beta3 = pm.Cauchy('beta3', alpha=0, beta=2.5, shape = 2)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1.0, shape = 2)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = tt.stack([alpha[0] + beta1[0]*X1 + beta2[0]*X2, \n",
    "                   alpha[1] + beta1[1]*X1 + beta2[1]*X2], axis = 1)\n",
    "    \n",
    "    order_means_potential = pm.Potential('order_means_potential', tt.switch(mu[1] - mu[0] > 0, -np.inf, 0))\n",
    "    \n",
    "    # weights\n",
    "#    BoundedWeights = pm.Bound(pm.Dirichlet, lower=0.1, upper = 0.9)\n",
    "   # BoundedWeights = pm.Bound(pm.Beta, lower=0.1, upper = 0.9)\n",
    "   # beta = BoundedWeights('beta', alpha = 7., beta = 3, shape = 2)\n",
    " #   w = np.array([beta, 1-beta])\n",
    "    w = pm.Dirichlet('w', np.array([1, 1]))\n",
    "\n",
    "    y_obs = pm.NormalMixture('y_obs', w, mu, sd=sigma, comp_shape = (2,), observed=Y)\n",
    "    \n",
    "#    step = pm.Metropolis()\n",
    "#    data = {'alpha': [-1.2, .5], 'beta1': [.5,.5], 'beta2': [.25, -.25], 'sigma': [.2, .8]}\n",
    "#    starting_point = dict(data)\n",
    "    trace_l = pm.sample(4000, init = 'advi_map', chains=1, nuts_kwargs=dict(target_accept=.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,3))\n",
    "ax = pm.traceplot(trace_l)\n",
    "#ax[0,0].set_xlim(-5,5)\n",
    "#ax[0,1].set_ylim(-3,3)\n",
    "#ax[1,0].set_xlim(-3,3)\n",
    "#ax[1,1].set_ylim(-.75,1.25)\n",
    "ax[2,0].set_xlim(-5,5)\n",
    "ax[2,1].set_ylim(-1,1)\n",
    "ax[3,0].set_xlim(-3,3)\n",
    "ax[3,1].set_ylim(-1,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nicest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to only look at 1 mode, or else you'll have huge ACFs!\n",
    "fig, ax = plt.subplots(2,2, figsize = (10,10))\n",
    "\n",
    "max_lag=500\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2['alpha'])[0][300:3000], ax = ax[0,0], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[0,0].set_ylabel('AC')\n",
    "ax[0,0].set_xlabel('Lag')\n",
    "ax[0,0].set_title('alpha')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2['beta1'])[0][300:3000], ax = ax[0,1], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[0,1].set_ylabel('AC')\n",
    "ax[0,1].set_xlabel('Lag')\n",
    "ax[0,1].set_title('beta1')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2['beta2'])[1][300:3000], ax = ax[1,0], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[1,0].set_ylabel('AC')\n",
    "ax[1,0].set_xlabel('Lag')\n",
    "ax[1,0].set_title('beta2')\n",
    "\n",
    "statsmodels.graphics.tsaplots.plot_acf(np.transpose(trace2['alpha'])[1][300:3000], ax = ax[1,1], use_vlines = True, lags = max_lag)\n",
    "#    ax[2].acorr(samples, normed = True)\n",
    "ax[1,1].set_ylabel('AC')\n",
    "ax[1,1].set_xlabel('Lag')\n",
    "ax[1,1].set_title('alphab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mixture_model_2comp:\n",
    "    ppc = pm.sample_posterior_predictive(trace2[300:3000][::100], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(Y, bins=20, normed=True,\n",
    "        histtype='step', lw=4,\n",
    "        label='Observed data');\n",
    "ax.hist(np.transpose(ppc['y_obs']).flatten(), color='orange', bins=20, normed=True,\n",
    "        histtype='step', lw=4,\n",
    "        label='Posterior predictive distribution');\n",
    "#ax.hist(ppc['y_obs'].mean(axis=0), color = 'black', bins=30, normed=True,\n",
    "#        histtype='step', lw=2, alpha = 1.0,\n",
    "#        label='Posterior predictive distribution');\n",
    "\n",
    "ax.legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than 2 mixture components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal, Beta\n",
    "from theano import tensor as tt\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    portion_remaining = tt.concatenate([[1], tt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "\n",
    "    return beta * portion_remaining\n",
    "\n",
    "K = 2\n",
    "\n",
    "with pm.Model() as mixture_model_Kcomp_b:\n",
    "    # Priors\n",
    "    alpha = pm.Cauchy('alpha', alpha=0, beta=1.25, shape = K)\n",
    "    beta1 = pm.Cauchy('beta1', alpha=0, beta=0.75, shape = K)\n",
    "    beta2 = pm.Cauchy('beta2', alpha=0, beta=0.75, shape = K)\n",
    "  #  beta3 = pm.Cauchy('beta3', alpha=0, beta=0.75, shape = 2)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=0.75, shape = K)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = tt.stack([alpha[i] + beta1[i]*X1 + beta2[i]*X2 for i in range(0,K) ], axis = 1)    \n",
    "   # order_means_potential = pm.Potential('order_means_potential', tt.switch(mu[1] - mu[0] < 0, -np.inf, 0) + tt.switch(mu[2] - mu[1] < 0, -np.inf, 0))\n",
    " #   order_means_potential2 = pm.Potential('order_means_potential_m2', tt.switch(mu[2] - mu[1] < 0, -np.inf, 0))\n",
    " #   order_means_potential3 = pm.Potential('order_means_potential_m3', tt.switch(mu[3] - mu[2] < 0, -np.inf, 0))\n",
    "    \n",
    "    # weights\n",
    " #   betadist = pm.Beta('betadist', 1.5, 1.5, shape=K)\n",
    " #   w = pm.Deterministic('w', stick_breaking(betadist))\n",
    "    w = pm.Dirichlet('w', np.ones(K))\n",
    "\n",
    "    y_obs = pm.NormalMixture('y_obs', w, mu, sd=sigma, comp_shape = (K,), observed=Y)\n",
    "    \n",
    "    traceK_b = pm.sample(10000, init = 'advi_map', chains=2, nuts_kwargs=dict(target_accept=.97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(traceK_b)\n",
    "ax[0,0].set_xlim(-5,5)\n",
    "ax[0,1].set_ylim(-3,3)\n",
    "ax[1,0].set_xlim(-3,3)\n",
    "ax[1,1].set_ylim(-.75,1.25)\n",
    "ax[2,0].set_xlim(-5,5)\n",
    "ax[2,1].set_ylim(-1,1)\n",
    "ax[3,0].set_xlim(0,3)\n",
    "ax[3,1].set_ylim(-1,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(traceK_b['alpha'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modedet(array):\n",
    "    '''\n",
    "    Takes trace of a pymc3 for a particular variable and returns the two solutions for it\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal, Beta\n",
    "from theano import tensor as tt\n",
    "\n",
    "def stick_breaking(beta):\n",
    "    portion_remaining = tt.concatenate([[1], tt.extra_ops.cumprod(1 - beta)[:-1]])\n",
    "\n",
    "    return beta * portion_remaining\n",
    "\n",
    "K = 2\n",
    "\n",
    "with pm.Model() as mixture_model_Kcomp:\n",
    "    # Priors\n",
    "    alpha = pm.Cauchy('alpha', alpha=0, beta=1.25, shape = K)\n",
    "    beta1 = pm.Cauchy('beta1', alpha=0, beta=0.75, shape = K)\n",
    "    beta2 = pm.Cauchy('beta2', alpha=0, beta=0.75, shape = K)\n",
    "  #  beta3 = pm.Cauchy('beta3', alpha=0, beta=0.75, shape = 2)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=0.75, shape = K)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = tt.stack([alpha[i] + beta1[i]*X1 + beta2[i]*X2 for i in range(0,K) ], axis = 1)    \n",
    "   # order_means_potential = pm.Potential('order_means_potential', tt.switch(mu[1] - mu[0] < 0, -np.inf, 0) + tt.switch(mu[2] - mu[1] < 0, -np.inf, 0))\n",
    " #   order_means_potential2 = pm.Potential('order_means_potential_m2', tt.switch(mu[2] - mu[1] < 0, -np.inf, 0))\n",
    " #   order_means_potential3 = pm.Potential('order_means_potential_m3', tt.switch(mu[3] - mu[2] < 0, -np.inf, 0))\n",
    "    \n",
    "    # weights\n",
    " #   betadist = pm.Beta('betadist', 1.5, 1.5, shape=K)\n",
    " #   w = pm.Deterministic('w', stick_breaking(betadist))\n",
    "    w = pm.Dirichlet('w', np.ones(K))\n",
    "\n",
    "    y_obs = pm.NormalMixture('y_obs', w, mu, sd=sigma, comp_shape = (K,), observed=Y)\n",
    "    \n",
    "    traceK = pm.sample(10000, init = 'advi_map', chains=2, nuts_kwargs=dict(target_accept=.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(traceK)\n",
    "ax[0,0].set_xlim(-5,5)\n",
    "ax[0,1].set_ylim(-3,3)\n",
    "ax[1,0].set_xlim(-3,3)\n",
    "ax[1,1].set_ylim(-.75,1.25)\n",
    "ax[2,0].set_xlim(-5,5)\n",
    "ax[2,1].set_ylim(-1,1)\n",
    "ax[3,0].set_xlim(0,3)\n",
    "ax[3,1].set_ylim(-1,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# long chain: do model comparison\n",
    "# try adding back beta3\n",
    "# try adding interactions amongst variables x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
